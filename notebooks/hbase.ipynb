{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env USERNAME=<username>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** Remember that for interacting with EDI Big Data Stack you must be authenticated at the system using kinit command. For more information, read the documentation at [Authenticating with Kerberos](https://docs.edincubator.eu/big-data-stack/basic-concepts.html#authenticating-with-kerberos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinit -kt ~/work/$USERNAME.service.keytab $USERNAME@EDINCUBATOR.EU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** before executing this notebookm you must follow create the HBase database following instructions at [Loading data into HBase](https://edincubator.eu/big-data-stack/tools/hbase.html#loading-data-into-hbase)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HBase\n",
    "## Loading data into HBase\n",
    "\n",
    "*HBaseLoadExample.java* contains the unique and main class of this MapReduce job. *HBaseLoadExample* class contains only the *HBaseWriterMapper* class, as this job doesn’t need a reducer.\n",
    "\n",
    "### HBaseWriterMapper\n",
    "\n",
    "```java\n",
    "public static class HBaseWriterMapper extends Mapper<Object, Text, ImmutableBytesWritable, Put> {\n",
    "\n",
    "      private long checkpoint = 100;\n",
    "      private long count = 0;\n",
    "\n",
    "      public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n",
    "          // Extract state using opencsv library\n",
    "          CSVReader reader = new CSVReader(new StringReader(value.toString()));\n",
    "          String[] line;\n",
    "\n",
    "          while ((line = reader.readNext()) != null) {\n",
    "              // Check that current line is not CSV's header\n",
    "              if (!line.equals(\"state\")) {\n",
    "                  context.setStatus(\"Creating row\");\n",
    "                  byte [] row = Bytes.toBytes(line[0]);\n",
    "                  Put put = new Put(row);\n",
    "\n",
    "                  // Insert info\n",
    "                  byte [] family = Bytes.toBytes(\"info\");\n",
    "\n",
    "                  // name\n",
    "                  byte [] qualifier = Bytes.toBytes(\"name\");\n",
    "                  byte [] hvalue = Bytes.toBytes(line[1]);\n",
    "                  put.addColumn(family, qualifier, hvalue);\n",
    "\n",
    "                  // neighborhood\n",
    "                  qualifier = Bytes.toBytes(\"neighborhood\");\n",
    "                  hvalue = Bytes.toBytes(line[2]);\n",
    "                  put.addColumn(family, qualifier, hvalue);\n",
    "\n",
    "                  // Same with address, city, state, postal_code, latitude,\n",
    "                  // longitude, is_open and categories\n",
    "                  [...]\n",
    "\n",
    "                  // Insert stats\n",
    "                  family = Bytes.toBytes(\"stats\");\n",
    "\n",
    "                  // stars\n",
    "                  qualifier = Bytes.toBytes(\"stars\");\n",
    "                  hvalue = Bytes.toBytes(line[9]);\n",
    "                  put.addColumn(family, qualifier, hvalue);\n",
    "\n",
    "                  // review_count\n",
    "                  qualifier = Bytes.toBytes(\"review_count\");\n",
    "                  hvalue = Bytes.toBytes(line[10]);\n",
    "                  put.addColumn(family, qualifier, hvalue);\n",
    "\n",
    "                  context.write(new ImmutableBytesWritable(row), put);\n",
    "\n",
    "                  // Set status every checkpoint lines for avoiding AM timeout\n",
    "                  if(++count % checkpoint == 0) {\n",
    "                      context.setStatus(\"Emitting Put \" + count);\n",
    "                  }\n",
    "              }\n",
    "          }\n",
    "      }\n",
    "  }\n",
    "```\n",
    "\n",
    "The *HBaseWriterMapper* class represents the mapper of our job. Its definition is very simple. It extends the *Mapper* class, receiving a tuple formed by a key of type Object and a value of type Text as input, and generating a tuple formed by a key of type *ImmutableBytesWritable* and a value of type Put as output.\n",
    "\n",
    "The map method is who processes the input and generates the output to be passed to the reducer. In this function, we take the value, representing a single CSV line and we create an object of type *org.apache.hadoop.hbase.client.Put*. This Put class represents a \"put\" action into the HBase database. Each column of the database must have a family, a qualifier and a value.\n",
    "\n",
    "### main & run\n",
    "\n",
    "At last, check main and run method of the *HBaseLoadExample* class.\n",
    "\n",
    "```java\n",
    "public int run(String[] otherArgs) throws Exception {\n",
    "      Configuration conf = getConf();\n",
    "\n",
    "      Job job = Job.getInstance(conf, \"HBase load example\");\n",
    "      job.setJarByClass(HBaseLoadExample.class);\n",
    "\n",
    "      FileInputFormat.setInputPaths(job, otherArgs[0]);\n",
    "      job.setInputFormatClass(TextInputFormat.class);\n",
    "      job.setMapperClass(HBaseWriterMapper.class);\n",
    "\n",
    "      TableMapReduceUtil.initTableReducerJob(\n",
    "              otherArgs[1],\n",
    "              null,\n",
    "              job\n",
    "      );\n",
    "      job.setNumReduceTasks(0);\n",
    "\n",
    "      return (job.waitForCompletion(true) ? 0 : 1);\n",
    "  }\n",
    "\n",
    "  public static void main(String [] args) throws Exception {\n",
    "      int status = ToolRunner.run(HBaseConfiguration.create(), new HBaseLoadExample(), args);\n",
    "      System.exit(status);\n",
    "  }\n",
    "```\n",
    "\n",
    "In the *run* method, the MapReduce job is configured. Concretely, in this example mapper class, input directories and output table (taken from the CLI when launching the job) are set.\n",
    "\n",
    "### pom.xml\n",
    "\n",
    "The *pom.xml* file compiles the project and generates the jar that we need to submit to EDI Big Data Stack.\n",
    "\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n",
    "       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
    "       xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n",
    "  <modelVersion>4.0.0</modelVersion>\n",
    "\n",
    "  <groupId>eu.edincubator.stack.examples</groupId>\n",
    "  <artifactId>hbaseexample</artifactId>\n",
    "  <version>1.0-SNAPSHOT</version>\n",
    "\n",
    "  <dependencies>\n",
    "      <dependency>\n",
    "          <groupId>org.apache.hadoop</groupId>\n",
    "          <artifactId>hadoop-mapreduce-client-core</artifactId>\n",
    "          <version>${hadoop.version}</version>\n",
    "          <scope>provided</scope>\n",
    "      </dependency>\n",
    "      <dependency>\n",
    "          <groupId>org.apache.hadoop</groupId>\n",
    "          <artifactId>hadoop-common</artifactId>\n",
    "          <version>${hadoop.version}</version>\n",
    "          <scope>provided</scope>\n",
    "      </dependency>\n",
    "      <dependency>\n",
    "          <groupId>com.opencsv</groupId>\n",
    "          <artifactId>opencsv</artifactId>\n",
    "          <version>4.1</version>\n",
    "      </dependency>\n",
    "      <dependency>\n",
    "          <groupId>org.apache.hbase</groupId>\n",
    "          <artifactId>hbase-common</artifactId>\n",
    "          <version>${hbase.version}</version>\n",
    "      </dependency>\n",
    "      <dependency>\n",
    "          <groupId>org.apache.hbase</groupId>\n",
    "          <artifactId>hbase-client</artifactId>\n",
    "          <version>${hbase.version}</version>\n",
    "      </dependency>\n",
    "      <dependency>\n",
    "          <groupId>org.apache.hbase</groupId>\n",
    "          <artifactId>hbase-protocol</artifactId>\n",
    "          <version>${hbase.version}</version>\n",
    "      </dependency>\n",
    "      <dependency>\n",
    "          <groupId>org.apache.hbase</groupId>\n",
    "          <artifactId>hbase-server</artifactId>\n",
    "          <version>${hbase.version}</version>\n",
    "      </dependency>\n",
    "      <dependency>\n",
    "          <groupId>org.apache.hbase</groupId>\n",
    "          <artifactId>hbase-thrift</artifactId>\n",
    "          <version>${hbase.version}</version>\n",
    "      </dependency>\n",
    "  </dependencies>\n",
    "\n",
    "  <properties>\n",
    "      <hadoop.version>3.0.0</hadoop.version>\n",
    "      <hbase.version>2.0.0</hbase.version>\n",
    "  </properties>\n",
    "</project>\n",
    "```\n",
    "\n",
    "Opposite to the pom.xml presented at MapReduce & YARN, this one doesn’t generate a \"fat jar\", so we have to add third party libraries (*com.opencsv*) when submitting the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling and submitting the job\n",
    "\n",
    "At first, you must create the java package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/work/examples/hbaseexample\n",
    "mvn clean package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before launching the job, we must download required third party libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir libjars\n",
    "cd libjars\n",
    "wget http://central.maven.org/maven2/com/opencsv/opencsv/4.1/opencsv-4.1.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, at stack-client docker cointainer, we can submit the job using the *hadoop jar* command. Notice the *-libjars* parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ..\n",
    "yarn jar target/hbaseexample-1.0-SNAPSHOT.jar eu.edincubator.stack.examples.hbase.HBaseLoadExample -libjars=libjars/opencsv-4.1.jar /samples/yelp/yelp_business/yelp_business.csv $USERNAME.yelp_business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we return to HBase shell, we can check that the table has been filled with data:\n",
    "\n",
    "```\n",
    "hbase(main):004:0> scan '<username>.yelp_business', {'LIMIT' => 5}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from Hbase\n",
    "\n",
    "In this example, we read the data previously loaded into HBase *yelp_business* table, compute it and write it into an HDFS folder. For that, we are going to reproduce the example shown at [MapReduce & YARN](map-reduce-yarn.ipynb), but reading data from HBase instead of a CSV file.\n",
    "\n",
    "This example is developed at *HBaseReadExample.java*. Its structure is similar to previous examples, even the reducer is the same reducer explained at [MapReduce & YARN](map-reduce-yarn.ipynb). The mapper is coded as follows:\n",
    "\n",
    "```java\n",
    "public static class HBaseReadMapper extends TableMapper<Text, IntWritable> {\n",
    "\n",
    "     private final static IntWritable one = new IntWritable(1);\n",
    "\n",
    "     public void map(ImmutableBytesWritable row, Result value, Context context) throws IOException, InterruptedException {\n",
    "         byte[] cell = value.getValue(Bytes.toBytes(\"info\"), Bytes.toBytes(\"state\"));\n",
    "         context.write(new Text(Bytes.toString(cell)), one);\n",
    "     }\n",
    " }\n",
    "```\n",
    "\n",
    "As you can notice, *HBaseReadMapper* extends from *org.apache.hadoop.hbase.mapreduce.TableMapper* instead of *org.apache.hadoop.mapreduce.Mapper*. In *TableMapper* class we only have to define output key and value types of the mapper, as input key and value types are fixed as they are read from HBase. map method receives a row id of *org.apache.hadoop.hbase.io.ImmutableBytesWritable* type and a value of type *org.apache.hadoop.hbase.client.Result*. Similar to the example shown at [MapReduce & YARN](map-reduce-yarn.ipynb), we take the value at column family info and qualifier state as output key and the value of one as output value. The reducer class is a replica of *StateSumReducer* that we coded at [MapReduce & YARN](map-reduce-yarn.ipynb), which aggregates all values for each key (state).\n",
    "\n",
    "### main & run\n",
    "\n",
    "```java\n",
    "public int run(String[] otherArgs) throws Exception {\n",
    "        Configuration conf = getConf();\n",
    "\n",
    "        Job job = Job.getInstance(conf, \"HBase read example\");\n",
    "        job.setJarByClass(HBaseReadExample.class);\n",
    "\n",
    "        Scan scan = new Scan();\n",
    "        scan.setCaching(500);\n",
    "        scan.setCacheBlocks(false);\n",
    "\n",
    "        TableMapReduceUtil.initTableMapperJob(\n",
    "                otherArgs[0],\n",
    "                scan,\n",
    "                HBaseReadMapper.class,\n",
    "                Text.class,\n",
    "                IntWritable.class,\n",
    "                job\n",
    "        );\n",
    "\n",
    "        job.setReducerClass(StateSumReducer.class);\n",
    "        job.setOutputKeyClass(Text.class);\n",
    "        job.setOutputValueClass(IntWritable.class);\n",
    "\n",
    "        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));\n",
    "\n",
    "        return (job.waitForCompletion(true) ? 0 : 1);\n",
    "    }\n",
    "\n",
    "    public static void main(String [] args) throws Exception {\n",
    "        int status = ToolRunner.run(HBaseConfiguration.create(), new HBaseReadExample(), args);\n",
    "        System.exit(status);\n",
    "  }\n",
    "```\n",
    "\n",
    "As can be seen, *run* method has some differences regarding to previous example. In this case, an instance of *org.apache.hadoop.hbase.client.Scan* class must be set for reading the database. In the same way, the mapper is set using the *initTableMapperJob* method from *org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil*. The reducer class is set in the same way as we saw in other examples.\n",
    "\n",
    "### Compiling and submitting the job\n",
    "\n",
    "The package is compiled as we saw in the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/work/examples/hbaseexample\n",
    "mvn clean package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, at stack-client docker cointainer, we can submit the job using the *hadoop jar* command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yarn jar target/hbaseexample-1.0-SNAPSHOT.jar eu.edincubator.stack.examples.hbase.HBaseReadExample $USERNAME.yelp_business /user/$USERNAME/hbase-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the output at HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/$USERNAME/hbase-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /user/$USERNAME/hbase-output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, those results are the same obtained at [MapReduce & YARN example](map-reduce-yarn.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Calysto Bash",
   "language": "bash",
   "name": "calysto_bash"
  },
  "language_info": {
   "file_extension": ".sh",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-sh",
   "name": "bash",
   "version": "0.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
