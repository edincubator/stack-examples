{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** Remember that for interacting with EDI Big Data Stack you must be authenticated at the system using kinit command. For more information, read the documentation at [Authenticating with Kerberos](https://docs.edincubator.eu/big-data-stack/basic-concepts.html#authenticating-with-kerberos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive\n",
    "\n",
    "Hive allows executing querying files stored at a distributed storage with a SQL-like language name HQL (Hive Query Language). In this tutorial we explain how to use Hive with sample files introduced in [HDFS](hdfs.ipynb) within EDI Big Data Stack.\n",
    "\n",
    "**Note:** If you need a new Hive database, you need to contact with [EDI Technical Support](https://docs.edincubator.eu/technical-support/index.html#technical-support) for creating the database and give you the proper permissions. For avoiding conflicts, all databases must follow the `username_databasename` convention.\n",
    "\n",
    "For interacting with Hive, the HiveQL kernel is available at Jupyter Lab. For connecting to the Hive database `<username>_yelp`, we must execute insert the following configuration at the first paragraph of the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$$ url=hive://<username>@edincubator-m-3-20191031113524.c.edi-call2.internal:10000/<username>_yelp\n",
    "$$ connect_args={\"auth\": \"KERBEROS\",\"kerberos_service_name\": \"hive\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Hive needs all permissions on datasetâ€™s folder, you must copy the *yelp_business* folder to your home directory.\n",
    "\n",
    "```bash\n",
    "hdfs dfs -cp /samples/yelp/yelp_business /user/<username>/\n",
    "```\n",
    "\n",
    "First we need to create table yelp_business. As we want to ingest CSV data, we are going to use [Hive CSV Serde](https://cwiki.apache.org/confluence/display/Hive/CSV+Serde):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE EXTERNAL TABLE IF NOT EXISTS yelp_business (\n",
    "  business_id string,\n",
    "  name string,\n",
    "  neighborhood string,\n",
    "  address string,\n",
    "  city string,\n",
    "  state string,\n",
    "  postal_code int,\n",
    "  latitude double,\n",
    "  longitude double,\n",
    "  stars float,\n",
    "  review_count int,\n",
    "  is_open boolean,\n",
    "  categories string\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "WITH SERDEPROPERTIES (\n",
    "   \"separatorChar\" = \",\",\n",
    "   \"quoteChar\"     = '\"',\n",
    "   \"escapeChar\"    = '\"'\n",
    ")\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/user/<username>/yelp_business'\n",
    "TBLPROPERTIES(\"skip.header.line.count\"=\"1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been imported from the CSV into the table you can execute SQL commands to query the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select business_id, name, city, state from yelp_business limit 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can execute SQL queries over the table. In our case, we want to get the ordered list of states with most businesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select state, count(state) as count from yelp_business group by state order by count desc;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HiveQL",
   "language": "hiveql",
   "name": "hiveql"
  },
  "language_info": {
   "codemirror_mode": "sql",
   "file_extension": ".hiveql",
   "mimetype": "text/x-hive",
   "name": "hive",
   "pygments_lexer": "postgresql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
