{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env USERNAME=<username>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** Remember that for interacting with EDI Big Data Stack you must be authenticated at the system using kinit command. For more information, read the documentation at [Authenticating with Kerberos](https://docs.edincubator.eu/big-data-stack/basic-concepts.html#authenticating-with-kerberos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "kinit -kt ~/work/$USERNAME.service.keytab $USERNAME@EDINCUBATOR.EU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce & YARN\n",
    "EDI Big Data Stack provides the MapReduce implementation over YARN. We have created a minimal example, based on [Yelp dataset](https://www.kaggle.com/yelp-dataset/yelp-dataset/version/6) that shows how to count how many Yelp businesses are in each USA state, and how to submit this MapReduce to EDI Big Data Stack.\n",
    "\n",
    "Yelp dataset is available for every user at */samples/yelp*. Open a terminal at Jyupyter Notebook and execute the following for inspecting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls -h /samples/yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find all examples at *~/work/examples* directory at your Jupyter Lab instance. A this section we will explain how to launch a typical map-reduce work over Yelp dataset. You can find this example project at *mrexample* folder. The relevant files at this project are *BusinessPerStateCount.java* and *pom.xml*. Later, we are going to inspect *BusinessPerStateCount.java* file.\n",
    "\n",
    "The *BusinessPerStateCount.java* file contains the unique and main class of this MapReduce job, the BusinessPerStateCount class, and two inner classes, *RowTokenizerMapper* and *StateSumReducer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RowTokenizerMapper\n",
    "\n",
    "```java\n",
    "public static class RowTokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {\n",
    "\n",
    "private final static IntWritable one = new IntWritable(1);\n",
    "\n",
    "public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n",
    "      // Extract state using opencsv library\n",
    "      CSVReader reader = new CSVReader(new StringReader(value.toString()));\n",
    "      String[] line;\n",
    "\n",
    "      while ((line = reader.readNext()) != null) {\n",
    "          // Check that current line is not CSV's header\n",
    "          if (!line.equals(\"state\")) {\n",
    "              // Write \"one\" for current state to context\n",
    "              context.write(new Text(line[5]), one);\n",
    "          }\n",
    "      }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "The *RowTokenizerMapper* class represents the mapper of our job. Its definition is very simple, as it only extends the base *Mapper* class, receiving a tuple formed by a key of type *Object* and a value of type Text as input, and generating a tuple formed by a key of type Text and a value of type IntWritable as output.\n",
    "\n",
    "The map method processes the input and generates the output that is passed passed to the reducer. In this function, we take the value, representing the state where the business is, and writes a tuple formed by the state as key, and a “one” as a value. This allow us grouping all appearances of a state in the reducer stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StateSumReducer\n",
    "\n",
    "```java\n",
    "public static class StateSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "\n",
    "  private IntWritable result = new IntWritable();\n",
    "\n",
    "  public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n",
    "      int sum = 0;\n",
    "      // For each state coincidence, +one\n",
    "      for (IntWritable val : values) {\n",
    "          sum += val.get();\n",
    "      }\n",
    "      result.set(sum);\n",
    "\n",
    "      // Return the state and the number of appearances.\n",
    "      context.write(key, result);\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "The *StateSumReducer* class represents the reducer stage of our job. Similar to the mapper, its definition states that it receives a tuple formed by key of type Text and a value of type IntWritable (generated by the mapper) and produces a tuple formed by key of type Text and a value of type IntWritable.\n",
    "\n",
    "The reduce function executes the logic of the reducer stage. It receives a key of type text and an *Iterable* of *IntWritables*. The MapReduce framework groups all tuples generated at *RowTokenizerMapper* by its keys, and stores the values for each key in a collection of *Iterable\\<IntWritable\\>* type. In the case of our example, for each value in the *Iterable* collection, we iterate the collection incrementing the counter obtaining the total count per key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main\n",
    "\n",
    "Finally, the *main* method of the *BusinessPerStateCount* class, which creates and configures the job, has the following code:\n",
    "\n",
    "```java\n",
    "public static void main(String [] args) throws IOException, ClassNotFoundException, InterruptedException {\n",
    "  Configuration conf = new Configuration();\n",
    "  Job job = Job.getInstance(conf, \"state count\");\n",
    "  job.setJarByClass(BusinessPerStateCount.class);\n",
    "\n",
    "  job.setMapperClass(RowTokenizerMapper.class);\n",
    "  job.setReducerClass(StateSumReducer.class);\n",
    "\n",
    "  job.setOutputKeyClass(Text.class);\n",
    "  job.setOutputValueClass(IntWritable.class);\n",
    "\n",
    "  FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "  FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "\n",
    "  System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "}\n",
    "```\n",
    "\n",
    "In the main method, the MapReduce job is configured. Concretely, this examples sets the mapper and reducer classes, the output key and value classes and the input and output directories (taken from the CLI when launching the job)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pom.xml \n",
    "\n",
    "The *pom.xml* file compiles the project and generates the jar that we need to submit to EDI Big Data Stack.\n",
    "\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n",
    "       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
    "       xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n",
    "  <modelVersion>4.0.0</modelVersion>\n",
    "\n",
    "  <groupId>eu.edincubator.stack.examples</groupId>\n",
    "  <artifactId>mr-example</artifactId>\n",
    "  <version>1.0-SNAPSHOT</version>\n",
    "\n",
    "  <build>\n",
    "      <plugins>\n",
    "          <plugin>\n",
    "              <artifactId>maven-assembly-plugin</artifactId>\n",
    "              <configuration>\n",
    "                  <archive>\n",
    "                      <manifest>\n",
    "                          <mainClass>eu.edincubator.stack.examples.mr.BusinessPerStateCount</mainClass>\n",
    "                      </manifest>\n",
    "                  </archive>\n",
    "                  <descriptorRefs>\n",
    "                      <descriptorRef>jar-with-dependencies</descriptorRef>\n",
    "                  </descriptorRefs>\n",
    "              </configuration>\n",
    "          </plugin>\n",
    "      </plugins>\n",
    "  </build>\n",
    "\n",
    "  <dependencies>\n",
    "      <dependency>\n",
    "          <groupId>org.apache.hadoop</groupId>\n",
    "          <artifactId>hadoop-mapreduce-client-core</artifactId>\n",
    "          <version>${hadoop.version}</version>\n",
    "          <scope>provided</scope>\n",
    "      </dependency>\n",
    "      <dependency>\n",
    "          <groupId>org.apache.hadoop</groupId>\n",
    "          <artifactId>hadoop-common</artifactId>\n",
    "          <version>${hadoop.version}</version>\n",
    "          <scope>provided</scope>\n",
    "      </dependency>\n",
    "      <dependency>\n",
    "          <groupId>com.opencsv</groupId>\n",
    "          <artifactId>opencsv</artifactId>\n",
    "          <version>4.1</version>\n",
    "      </dependency>\n",
    "  </dependencies>\n",
    "\n",
    "  <properties>\n",
    "      <hadoop.version>2.7.3</hadoop.version>\n",
    "  </properties>\n",
    "</project>\n",
    "```\n",
    "\n",
    "This file contains two important parts. The fist one, is the *\\<build\\>* block. This block stablished how the jar is going to be built. In our case, we have choose to create a \"fat jar\" including the third party dependencies (*com.opencsv* library). On the other hand, the *\\<dependencies\\>* block contains the dependencies of our project. It is important to import the correct version of the libraries. For more information check [Tools and versions](https://docs.edincubator.eu/big-data-stack/architecture.html#tools-and-versions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling and submitting the job\n",
    "\n",
    "First, from a JupyterLab terminal, you must create the java package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~/work/examples/mrexample\n",
    "mvn clean compile assembly:single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~/work/examples/mrexample\n",
    "yarn jar target/mr-example-1.0-SNAPSHOT-jar-with-dependencies.jar /samples/yelp/yelp_business/yelp_business.csv /user/$USERNAME/state-count-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the job is successfully executed, the result is written to the `/user/<username>/state-count-output` directory. In case of any problem during its execution, the error will be printed to the console. For further details about the job, you can check the ResourceManager UI at [https://edi-master.novalocal:8443/gateway/hdp/yarnuiv2].\n",
    "\n",
    "TODO: update URL\n",
    "\n",
    "Finally, if you check the output directory, you will see the result of the job as a part-r-00000 file. The execution of this job generated a single file because only one reducer is executed. However, the output could be split into different files if more reducers were required to perform the job.\n",
    "\n",
    "Then, we can list the files inside the output directory and print, directly to the console, the contents of the generated file. The `-cat` parameter shows the contents of the file, showing the number of businesses for each USA state obtained as the result of the map reduce job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user/$USERNAME/state-count-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -cat /user/$USERNAME/state-count-output/part-r-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
